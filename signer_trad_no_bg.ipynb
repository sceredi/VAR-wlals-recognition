{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T13:01:52.898284Z",
     "start_time": "2025-04-01T13:01:50.452604Z"
    }
   },
   "source": [
    "import itertools\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "from handcrafted.app.dataset.dataset_loader import DatasetLoader\n",
    "from handcrafted.app.dataset.utils.dataset_creator import DatasetCreator\n",
    "from handcrafted.app.dataset.utils.frames_splitter import FramesSplitter\n",
    "from handcrafted.app.model.model_statistics import ModelStatistics"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 15:01:51.238111: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 15:01:51.239602: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 15:01:51.260250: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-01 15:01:51.260288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-01 15:01:51.261155: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 15:01:51.264849: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-01 15:01:51.265596: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-01 15:01:51.905114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "1f091fd4fd7fd7f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:02:04.677285Z",
     "start_time": "2025-04-01T13:01:52.903280Z"
    }
   },
   "source": [
    "loader = DatasetLoader(directory=\"./data/frames_no_bg/\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12023it [00:11, 1021.67it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "67198a3f4fbcc8c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:02:04.777630Z",
     "start_time": "2025-04-01T13:02:04.775325Z"
    }
   },
   "source": [
    "num_signers = 8\n",
    "if num_signers == -1:\n",
    "    signers = loader.signers\n",
    "    num_signers = loader.num_signers\n",
    "else:\n",
    "    signers = dict(itertools.islice(loader.signers.items(), num_signers))\n",
    "splitter = FramesSplitter(signers, val_split=0.3, test_split=0.3, frames_to_extract=1000, seed=42)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:02:04.826256Z",
     "start_time": "2025-04-01T13:02:04.823696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for signer in signers.values():\n",
    "    signer_frames = []\n",
    "    print(f\"Signer {signer.id} has {len(signer.videos)} videos\")"
   ],
   "id": "17a437d4d7310b0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signer 117 has 28 videos\n",
      "Signer 26 has 93 videos\n",
      "Signer 63 has 39 videos\n",
      "Signer 35 has 55 videos\n",
      "Signer 42 has 34 videos\n",
      "Signer 48 has 6 videos\n",
      "Signer 104 has 4 videos\n",
      "Signer 70 has 14 videos\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "99bffccbd218b30c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:02:04.978091Z",
     "start_time": "2025-04-01T13:02:04.928510Z"
    }
   },
   "source": "X_train, X_train_aug, y_train, X_val, X_val_aug, y_val, X_test, X_test_aug, y_test = splitter.split(X_content=lambda f: f)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 542.74it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "107083a87bfe968f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:02:05.036820Z",
     "start_time": "2025-04-01T13:02:05.033363Z"
    }
   },
   "source": [
    "# Get the total of different labels\n",
    "num_classes = num_signers\n",
    "print(num_classes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "fbe3d8897565f4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:02:05.118651Z",
     "start_time": "2025-04-01T13:02:05.112277Z"
    }
   },
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.fit_transform(y_val)\n",
    "y_test_encoded = label_encoder.fit_transform(y_test)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "931f564da905c84d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:02:05.201722Z",
     "start_time": "2025-04-01T13:02:05.161090Z"
    }
   },
   "source": [
    "BATCH_SIZE = 4096\n",
    "dataset_creator = DatasetCreator()\n",
    "train_dataset = dataset_creator.create_custom_dataset(X_train, y_train_encoded, augmentation = X_train_aug, batch_size=BATCH_SIZE)\n",
    "val_dataset = dataset_creator.create_custom_dataset(X_val, y_val_encoded,  augmentation=X_val_aug, batch_size=BATCH_SIZE)\n",
    "test_dataset = dataset_creator.create_custom_dataset(X_test, y_test_encoded, augmentation=X_test_aug, batch_size=BATCH_SIZE)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 83886.08it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 91180.52it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 104857.60it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "44c26ae96458cfaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:02:05.213038Z",
     "start_time": "2025-04-01T13:02:05.210870Z"
    }
   },
   "source": [
    "xgb_params = {\n",
    "    \"objective\":\"multi:softmax\",\n",
    "    \"num_class\":num_classes,\n",
    "    \"eval_metric\":\"mlogloss\",\n",
    "    \"max_depth\":6,\n",
    "    \"learning_rate\":0.1,\n",
    "    \"n_estimators\":100,\n",
    "    \"subsample\":0.8,\n",
    "    \"colsample_bytree\":0.8,\n",
    "    \"gamma\":0,\n",
    "    \"min_child_weight\":1,\n",
    "    \"reg_lambda\":1,\n",
    "    \"reg_alpha\":0,\n",
    "    \"tree_method\":\"hist\", # Change to \"gpu_hist\" for GPU acceleration\n",
    "    # \"device\":\"cuda\", # Use GPU\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ffcafd541c2b8a3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T13:07:29.087568Z",
     "start_time": "2025-04-01T13:02:05.254465Z"
    }
   },
   "source": [
    "xgb_model = None\n",
    "for train_batch in tqdm(val_dataset):\n",
    "    X_batch, y_batch = train_batch.load(shuffle=True)\n",
    "    dtrain = xgb.DMatrix(X_batch, label=y_batch)\n",
    "    xgb_model = xgb.train(params=xgb_params, dtrain=dtrain, num_boost_round=10, xgb_model=xgb_model)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [05:23<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m xgb_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train_batch \u001B[38;5;129;01min\u001B[39;00m tqdm(val_dataset):\n\u001B[0;32m----> 3\u001B[0m     X_batch, y_batch \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     dtrain \u001B[38;5;241m=\u001B[39m xgb\u001B[38;5;241m.\u001B[39mDMatrix(X_batch, label\u001B[38;5;241m=\u001B[39my_batch)\n\u001B[1;32m      5\u001B[0m     xgb_model \u001B[38;5;241m=\u001B[39m xgb\u001B[38;5;241m.\u001B[39mtrain(params\u001B[38;5;241m=\u001B[39mxgb_params, dtrain\u001B[38;5;241m=\u001B[39mdtrain, num_boost_round\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, xgb_model\u001B[38;5;241m=\u001B[39mxgb_model)\n",
      "File \u001B[0;32m~/uni-lab/VAR/wlasl-recognition/handcrafted/app/dataset/utils/dataset_creator.py:160\u001B[0m, in \u001B[0;36mMiniBatch.load\u001B[0;34m(self, shuffle)\u001B[0m\n\u001B[1;32m    158\u001B[0m     ims \u001B[38;5;241m=\u001B[39m ims \u001B[38;5;241m+\u001B[39m augs\n\u001B[1;32m    159\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f, _ \u001B[38;5;129;01min\u001B[39;00m ims:\n\u001B[0;32m--> 160\u001B[0m         features\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_extract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    161\u001B[0m         lbl\u001B[38;5;241m.\u001B[39mappend(label)\n\u001B[1;32m    162\u001B[0m features \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(features)\n",
      "File \u001B[0;32m~/uni-lab/VAR/wlasl-recognition/handcrafted/app/dataset/utils/dataset_creator.py:136\u001B[0m, in \u001B[0;36mMiniBatch._extract_features\u001B[0;34m(frame)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_extract_features\u001B[39m(frame: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[0;32m--> 136\u001B[0m     hog_features, _ \u001B[38;5;241m=\u001B[39m \u001B[43mHOGExtractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess_frames\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m     lbp_features \u001B[38;5;241m=\u001B[39m LBPExtractor([frame])\u001B[38;5;241m.\u001B[39mget_lbp_features()\n\u001B[1;32m    138\u001B[0m     color_hist_features \u001B[38;5;241m=\u001B[39m ColorHistogram([frame], n_bins\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m)\u001B[38;5;241m.\u001B[39mprocess_frames(\n\u001B[1;32m    139\u001B[0m         cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2HSV, separate_colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, normalize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    140\u001B[0m     )\n",
      "File \u001B[0;32m~/uni-lab/VAR/wlasl-recognition/handcrafted/app/features/extractor/hog_extractor.py:59\u001B[0m, in \u001B[0;36mHOGExtractor.process_frames\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     57\u001B[0m processed_features \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m frame \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframes:\n\u001B[0;32m---> 59\u001B[0m     hog_features, hog_image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_extract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m     processed_frames\u001B[38;5;241m.\u001B[39mappend(hog_image)\n\u001B[1;32m     61\u001B[0m     processed_features\u001B[38;5;241m.\u001B[39mappend(hog_features)\n",
      "File \u001B[0;32m~/uni-lab/VAR/wlasl-recognition/handcrafted/app/features/extractor/hog_extractor.py:78\u001B[0m, in \u001B[0;36mHOGExtractor._extract\u001B[0;34m(self, frame)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Extract HOG features from a single frame.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \n\u001B[1;32m     67\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;124;03m    Tuple containing the HOG features and the processed frame.\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     77\u001B[0m gray_frame \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(frame, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2GRAY)\n\u001B[0;32m---> 78\u001B[0m hog_features, hog_image \u001B[38;5;241m=\u001B[39m \u001B[43mhog\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgray_frame\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhog_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m hog_image \u001B[38;5;241m=\u001B[39m (hog_image \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m255\u001B[39m)\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39muint8)\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m hog_features, hog_image\n",
      "File \u001B[0;32m~/uni-lab/VAR/wlasl-recognition/.venv/lib/python3.10/site-packages/skimage/_shared/utils.py:316\u001B[0m, in \u001B[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    313\u001B[0m channel_axis \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchannel_axis\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m channel_axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 316\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    318\u001B[0m \u001B[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001B[39;00m\n\u001B[1;32m    319\u001B[0m \u001B[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001B[39;00m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001B[39;00m\n\u001B[1;32m    321\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39misscalar(channel_axis):\n",
      "File \u001B[0;32m~/uni-lab/VAR/wlasl-recognition/.venv/lib/python3.10/site-packages/skimage/feature/_hog.py:288\u001B[0m, in \u001B[0;36mhog\u001B[0;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, channel_axis)\u001B[0m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_blocks_col):\n\u001B[1;32m    286\u001B[0m         block \u001B[38;5;241m=\u001B[39m orientation_histogram[r:r \u001B[38;5;241m+\u001B[39m b_row, c:c \u001B[38;5;241m+\u001B[39m b_col, :]\n\u001B[1;32m    287\u001B[0m         normalized_blocks[r, c, :] \u001B[38;5;241m=\u001B[39m \\\n\u001B[0;32m--> 288\u001B[0m             \u001B[43m_hog_normalize_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblock\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblock_norm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;124;03mThe final step collects the HOG descriptors from all blocks of a dense\u001B[39;00m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;124;03moverlapping grid of blocks covering the detection window into a combined\u001B[39;00m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;124;03mfeature vector for use in the window classifier.\u001B[39;00m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m feature_vector:\n",
      "File \u001B[0;32m~/uni-lab/VAR/wlasl-recognition/.venv/lib/python3.10/site-packages/skimage/feature/_hog.py:17\u001B[0m, in \u001B[0;36m_hog_normalize_block\u001B[0;34m(block, method, eps)\u001B[0m\n\u001B[1;32m     15\u001B[0m     out \u001B[38;5;241m=\u001B[39m block \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(np\u001B[38;5;241m.\u001B[39msum(block \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m) \u001B[38;5;241m+\u001B[39m eps \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     16\u001B[0m     out \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mminimum(out, \u001B[38;5;241m0.2\u001B[39m)\n\u001B[0;32m---> 17\u001B[0m     out \u001B[38;5;241m=\u001B[39m out \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39msqrt(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;241m+\u001B[39m eps \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSelected block normalization method is invalid.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/nix/store/4jc5dxnq87rs05vnifdyk2fy1mraz63l-python3-3.10.13-env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2313\u001B[0m, in \u001B[0;36msum\u001B[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[1;32m   2310\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m   2311\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\u001B[0;32m-> 2313\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapreduction\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msum\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkeepdims\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2314\u001B[0m \u001B[43m                      \u001B[49m\u001B[43minitial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwhere\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/nix/store/4jc5dxnq87rs05vnifdyk2fy1mraz63l-python3-3.10.13-env/lib/python3.10/site-packages/numpy/core/fromnumeric.py:88\u001B[0m, in \u001B[0;36m_wrapreduction\u001B[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     86\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m reduction(axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpasskwargs)\n\u001B[0;32m---> 88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mufunc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpasskwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "e4741d3f0bd140a9",
   "metadata": {},
   "source": [
    "y_pred = []\n",
    "y_test_full = []\n",
    "for test_batch in tqdm(test_dataset):\n",
    "    X_batch, y_batch = test_batch.load(shuffle=True)\n",
    "    y_test_full.extend(y_batch)\n",
    "    dtest = xgb.DMatrix(X_batch)\n",
    "    y_pred_batch = xgb_model.predict(dtest)\n",
    "    y_pred.extend(y_pred_batch)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "374f709d5e515120",
   "metadata": {},
   "source": [
    "stats = ModelStatistics(save_name=f\"svc_signer_test_{len(y_test_full)}\", save_dir=\"signer/plots\")\n",
    "stats.print_accuracy(y_test_full, y_pred)\n",
    "stats.plot_confusion_matrix(y_test_full, y_pred, save=True, plot=True)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
